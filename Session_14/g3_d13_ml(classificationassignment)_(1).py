# -*- coding: utf-8 -*-
"""G3_d13_ML(ClassificationAssignment) (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GT3bQWXWd_BxnTNels3LhgY6a38KDCeC
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df = pd.read_csv('https://github.com/bipulshahi/Dataset/raw/refs/heads/main/Churn_Modelling.csv')
df.head()

"""* Use the data to perform EDA (Exploratory data analysis)
* Then build a predictive model to predict the churn (Exit)
"""

df.shape

df.columns

df1 = df.drop(columns = ['RowNumber', 'CustomerId', 'Surname'])
df1.head()

df1.dtypes

df1.isna().sum()

df1['CreditScore'].plot.hist()

df1['CreditScore'].plot.box()

df1['Geography'].value_counts()

df1['Geography'].value_counts().plot.bar()

df1['Gender'].value_counts().plot.bar()

df1['Exited'].value_counts()

df1['Exited'].value_counts().plot.bar()

df1.head(3)

df1['Age'].plot.hist()

df1['Age'].plot.box()

#%ge of data with age outliers
q1 = df1['Age'].quantile(0.25)
q3 = df1['Age'].quantile(0.75)

iqr = q3 - q1
uf = q3 + 1.5 * iqr
lf = q1 - 1.5 * iqr

print("%ge of data as outliers")
print((((df1['Age'] > uf) | (df1['Age'] < lf)).sum()/10000)*100)

df1.head(3)

df1['Tenure'].nunique()

df1['Tenure'].value_counts()

df1['Balance'].plot.hist()

df1['Balance'].plot.box()

df1['NumOfProducts'].value_counts().plot.bar()

df1['HasCrCard'].value_counts().plot.bar()

df1['IsActiveMember'].value_counts().plot.bar()

df1['EstimatedSalary'].plot.hist()

df1.head(3)

#how 'Geography' is affecting 'Exited'
pd.crosstab(df1['Geography'] , df1['Exited'])

pd.crosstab(df1['Geography'] , df1['Exited']).plot.bar()

#how 'Gender' is affecting 'Exited'
pd.crosstab(df1['Gender'] , df1['Exited'])

pd.crosstab(df1['Gender'] , df1['Exited']).plot.bar()

#how 'Tenure' is affecting 'Exited'
pd.crosstab(df1['Tenure'] , df1['Exited']).plot.bar()

#how 'NumOfProducts' is affecting 'Exited'
pd.crosstab(df1['NumOfProducts'] , df1['Exited']).plot.bar()

#how 'HasCrCard' is affecting 'Exited'
pd.crosstab(df1['HasCrCard'] , df1['Exited']).plot.bar()

#how 'IsActiveMember 	' is affecting 'Exited'
pd.crosstab(df1['IsActiveMember'] , df1['Exited']).plot.bar()

df1.head(3)

#CreditScore vs Exited
df1.groupby('Exited')['CreditScore'].agg(['mean'])

#Age vs Exited
df1.groupby('Exited')['Age'].agg(['mean'])

#Balance vs Exited
df1.groupby('Exited')['Balance'].agg(['mean'])

#EstimatedSalary vs Exited
df1.groupby('Exited')['EstimatedSalary'].agg(['mean'])

df1.head(3)

df1.dtypes

#df1['Geography'] = df1['Geography'].astype('category')
#df1['Gender'] = df1['Gender'].astype('category')

df1.head()

#Label Encoding foe gender
df1['Gender']=df1['Gender'].map({"Female":0 , "Male":1})

df1.head(3)

#one hot encoding for Geography
df2 = pd.get_dummies(df1, columns=['Geography'] , dtype = int)

X = df2.drop(columns = ['Exited'])
y = df2['Exited']

y.value_counts()

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

scaler.fit(X)
Xscaled = scaler.transform(X)

#split
from sklearn.model_selection import train_test_split
xtrain, xtest, ytrain, ytest = train_test_split(Xscaled,y)

#build model
from sklearn.neighbors import KNeighborsClassifier
modelA = KNeighborsClassifier()
modelA.fit(xtrain,ytrain)

#evaluate model performance - model accuracy
print("Model accuracy on training data -",modelA.score(xtrain,ytrain))
print("Model accuracy on test data -",modelA.score(xtest,ytest))

"""**Handling and working with biased data**"""

#predicted values on data used for training
ytrainP = modelA.predict(xtrain)
ytestP = modelA.predict(xtest)

from sklearn.metrics import confusion_matrix, classification_report

print(confusion_matrix(ytrain , ytrainP))

print(classification_report(ytrain , ytrainP))

print(confusion_matrix(ytest , ytestP))

print(classification_report(ytest , ytestP))

"""**Undersampling to fix biasness of the model**"""

X = df2.drop(columns = ['Exited'])
y = df2['Exited']

y.value_counts()

from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler()

Xu,yu = rus.fit_resample(X,y)

yu.value_counts()

scaler = MinMaxScaler()

scaler.fit(X)
Xscaled = scaler.transform(Xu)

xtrain, xtest, ytrain, ytest = train_test_split(Xscaled,yu,stratify=yu)

ytrain.value_counts()

ytest.value_counts()

modelB = KNeighborsClassifier()
modelB.fit(xtrain,ytrain)

#evaluate model performance - model accuracy
print("Model accuracy on training data -",modelB.score(xtrain,ytrain))
print("Model accuracy on test data -",modelB.score(xtest,ytest))

print(classification_report(ytrain, modelB.predict(xtrain)))

print(classification_report(ytest, modelB.predict(xtest)))

"""**Handling biasness using oversampling**"""

X = df2.drop(columns = ['Exited'])
y = df2['Exited']

y.value_counts()

#from imblearn.over_sampling import RandomOverSampler
from imblearn.over_sampling import SMOTE          #Synthetic minority oversampling technique estimation
ros = SMOTE()

Xo,yo = ros.fit_resample(X,y)

yo.value_counts()

scaler = MinMaxScaler()

scaler.fit(X)
Xscaled = scaler.transform(Xo)

xtrain, xtest, ytrain, ytest = train_test_split(Xscaled,yo,stratify=yo)

print(ytrain.value_counts())
print(ytest.value_counts())

modelC = KNeighborsClassifier()
modelC.fit(xtrain,ytrain)

#evaluate model performance - model accuracy
print("Model accuracy on training data -",modelC.score(xtrain,ytrain))
print("Model accuracy on test data -",modelC.score(xtest,ytest))

print(classification_report(ytrain, modelC.predict(xtrain)))

print(classification_report(ytest, modelC.predict(xtest)))

"""**Logistic Regression**"""

X = df2.drop(columns = ['Exited'])
y = df2['Exited']

scaler = MinMaxScaler()

scaler.fit(X)
Xscaled = scaler.transform(X)

xtrain, xtest, ytrain, ytest = train_test_split(Xscaled,y,stratify=y)

from sklearn.linear_model import LogisticRegression
model_log_A = LogisticRegression()

model_log_A.fit(xtrain,ytrain)

#evaluate model performance - model accuracy
print("Model accuracy on training data -",model_log_A.score(xtrain,ytrain))
print("Model accuracy on test data -",model_log_A.score(xtest,ytest))

from sklearn.metrics import classification_report

print(classification_report(ytrain, model_log_A.predict(xtrain)))

print(classification_report(ytest, model_log_A.predict(xtest)))

"""**Evaluate the model performance after oversampling**"""

X = df2.drop(columns = ['Exited'])
y = df2['Exited']

from imblearn.over_sampling import SMOTE
ros = SMOTE()

Xo,yo = ros.fit_resample(X,y)

scaler = MinMaxScaler()

scaler.fit(Xo)
Xscaled = scaler.transform(Xo)

xtrain, xtest, ytrain, ytest = train_test_split(Xscaled,yo,stratify=yo)

from sklearn.linear_model import LogisticRegression
model_log_B = LogisticRegression()

model_log_B.fit(xtrain,ytrain)

#evaluate model performance - model accuracy
print("Model accuracy on training data -",model_log_B.score(xtrain,ytrain))
print("Model accuracy on test data -",model_log_B.score(xtest,ytest))

print(classification_report(ytrain, model_log_B.predict(xtrain)))

print(classification_report(ytest, model_log_B.predict(xtest)))

"""**Handling Biasness using Class weight management**"""

x = 1
w0 = 1e-2
w1 = 1e-2

z = 1/(1 + np.exp(-(w0 + w1*x)))
print(z)

x = 2
w0 = 4 * 1e-2
w1 = 4 * 1e-2

z = 1/(1 + np.exp(-(w0 + w1*x)))
print(z)

X = df2.drop(columns = ['Exited'])
y = df2['Exited']

scaler = MinMaxScaler()

scaler.fit(X)
Xscaled = scaler.transform(X)

xtrain, xtest, ytrain, ytest = train_test_split(Xscaled,y,stratify=y)

from sklearn.linear_model import LogisticRegression
model_log_D = LogisticRegression(class_weight={0:1 , 1:6})

model_log_D.fit(xtrain,ytrain)

#evaluate model performance - model accuracy
print("Model accuracy on training data -",model_log_D.score(xtrain,ytrain))
print("Model accuracy on test data -",model_log_D.score(xtest,ytest))

print(classification_report(ytrain, model_log_D.predict(xtrain)))

print(classification_report(ytest, model_log_D.predict(xtest)))

print(model_log_D.coef_)

