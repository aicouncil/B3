# -*- coding: utf-8 -*-
"""g3_d16_classification(decisiontreeclassifier).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z9031wNz5O7KWWn_Zsr-d9AeBFwWPmeK
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df = pd.read_csv('https://github.com/bipulshahi/Dataset/raw/refs/heads/main/Loan%20Approval%20Prediction.csv')
df.head()

df1 = df.drop(columns = 'Loan_ID')
df1.head()

df1.isna().sum()

num_columns = df1.select_dtypes(include = 'number').columns
print(num_columns)

non_num_columns = df1.select_dtypes(exclude = 'number').columns
print(non_num_columns)

df1[non_num_columns].isna().sum()

df1.shape

print(df1['Gender'].unique())
print(df1['Married'].unique())
print(df1['Dependents'].unique())
print(df1['Self_Employed'].unique())

df1['Gender'].value_counts()

print(df1['Gender'].mode()[0])
print(df1['Married'].mode()[0])
print(df1['Dependents'].mode()[0])
print(df1['Self_Employed'].mode()[0])

df1['Gender'] = df1['Gender'].fillna(df1['Gender'].mode()[0])
df1['Married'] = df1['Married'].fillna(df1['Married'].mode()[0])
df1['Dependents'] = df1['Dependents'].fillna(df1['Dependents'].mode()[0])
df1['Self_Employed'] = df1['Self_Employed'].fillna(df1['Self_Employed'].mode()[0])

df1[num_columns].isna().sum()

print(df1['LoanAmount'].agg(['mean' , 'median']))
print(df1['Loan_Amount_Term'].agg(['mean' , 'median']))
print(df1['Credit_History'].agg(['mean' , 'median']))

df1['LoanAmount'] = df1['LoanAmount'].fillna(df1['LoanAmount'].median())
df1['Loan_Amount_Term'] = df1['Loan_Amount_Term'].fillna(df1['LoanAmount'].median())
df1['Credit_History'] = df1['Credit_History'].fillna(df1['LoanAmount'].median())

df1.isna().sum()

df1.head(3)

df2 = df1.copy()

income = df2['ApplicantIncome'] + df2['CoapplicantIncome']

#insert() - a method to insert a data column into the data frame
df2.insert(5, "Income" , income)

df2.head(3)

df2 = df2.drop(columns = ['ApplicantIncome', 	'CoapplicantIncome'])
df2.head()

non_num_columns

for col in non_num_columns:
  print(df2[col].unique())

df2['Gender'] = df2['Gender'].map({'Male':0, 'Female':1})
df2['Married'] = df2['Married'].map({"No":0, "Yes":1})
df2['Dependents'] = df2['Dependents'].map({'0':0, '1':1, '2':2, '3+':3})
df2['Education'] = df2['Education'].map({'Not Graduate':0 , 'Graduate':1})
df2['Self_Employed'] = df2['Self_Employed'].map({"No":0 , "Yes":1})
df2['Property_Area'] = df2['Property_Area'].map({'Rural':0, 'Semiurban':1, 'Urban':2})
df2['Loan_Status'] = df2['Loan_Status'].map({"N":0 , "Y":1})

df2.head()

X = df2.drop(columns = 'Loan_Status')
y = df2['Loan_Status']

y.value_counts()

from imblearn.over_sampling import SMOTE
ros = SMOTE()

Xr,yr = ros.fit_resample(X,y)

yr.value_counts()

from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest = train_test_split(Xr,yr)

from sklearn.tree import DecisionTreeClassifier
modelA = DecisionTreeClassifier(criterion = 'entropy' , max_depth=3 , class_weight = {0:2 , 1:1} , max_leaf_nodes=8)

modelA.fit(xtrain, ytrain)

print(modelA.score(xtrain,ytrain))
print(modelA.score(xtest,ytest))

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
plt.figure(figsize = (16,14))

plot_tree(modelA, feature_names=X.columns)

plt.show()

from sklearn.metrics import classification_report

print(classification_report(ytrain , modelA.predict(xtrain)))

print(classification_report(ytest , modelA.predict(xtest)))

"""**Cross validation**"""

#5 - fold cross validation - split data for 5 times and evaluate its performance

from sklearn.model_selection import cross_val_score
modelB = DecisionTreeClassifier(criterion = 'entropy' , max_depth=3 , class_weight = {0:2 , 1:1} , max_leaf_nodes=8)
scores = cross_val_score(modelB, X, y, cv=10)
print(scores)
print()
print("Average scores" , scores.mean())

"""**Grid Seach CV**"""

from sklearn.model_selection import GridSearchCV

params = {
    "criterion" : ["gini", "entropy", "log_loss"],
    "splitter" : ["best", "random"],
    "max_depth" : [2,4,6,7],
    "min_samples_leaf" : [2,5,7,10],
    "max_leaf_nodes" : [3,5,6,7]
}

modelC = DecisionTreeClassifier()

gridmodel = GridSearchCV(modelC, params)

gridmodel.fit(X,y)

gridmodel.best_params_

gridmodel.best_estimator_

model_after_grid = DecisionTreeClassifier(criterion = 'entropy' , max_depth=2 ,
                                          class_weight = {0:2 , 1:1} , max_leaf_nodes=3,
                                          min_samples_leaf=2)

model_after_grid.fit(xtrain,ytrain)

print(model_after_grid.score(xtrain,ytrain))
print(model_after_grid.score(xtest,ytest))

print(classification_report(ytrain , model_after_grid.predict(xtrain)))

print(classification_report(ytest , model_after_grid.predict(xtest)))

"""**Predictions**"""

print(model_after_grid.predict([xtest.values[10]]))

"""**Random Forest Classifier**"""

from sklearn.ensemble import RandomForestClassifier

model_r = RandomForestClassifier(n_estimators=100 , criterion = 'gini' , max_depth=5,
                                 class_weight = {0 : 1.75 , 1 : 1})

model_r.fit(xtrain,ytrain)

print(model_r.score(xtrain,ytrain))

print(model_r.score(xtest,ytest))

print(classification_report(ytrain , model_r.predict(xtrain)))

print(classification_report(ytest , model_r.predict(xtest)))

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
plt.figure(figsize = (16,14))

plot_tree(model_r[99], feature_names=X.columns)

plt.show()

"""**Predictions**- 10th sample in test data"""

print(model_r.predict([xtest.values[10]]))

all_predictions = []
for i in range(0,100):
  all_predictions.append(model_r[i].predict([xtest.values[10]]))

pd.Series(all_predictions).value_counts()

